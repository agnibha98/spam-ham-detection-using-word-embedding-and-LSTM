# -*- coding: utf-8 -*-
"""spam detection kaggle uci dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/179AlR9P3V6UbmrYGt__wM1RXjfO3Zakk
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from google.colab import files
uploaded=files.upload()

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense,Input,GlobalMaxPooling1D,LSTM,Embedding
from tensorflow.keras.models import  Model

import io
df = pd.read_csv(io.BytesIO(uploaded['spam.csv']),encoding='ISO-8859-1')

df.head()

df=df.drop(["Unnamed: 2","Unnamed: 3","Unnamed: 4"],axis=1)

df.head()

df.columns=("Labels","Messege")

df.head()

df['Binary Labels']=df['Labels'].map({'ham':0,'spam':1})

df.head()

x_train,x_test,y_train,y_test=train_test_split(df['Messege'],df['Binary Labels'],test_size=0.2)


MAX_VOCAB_SIZE=20000
tokenizer=Tokenizer(num_words=MAX_VOCAB_SIZE)
tokenizer.fit_on_texts(x_train) #don't fit the test inputs
train_sequences=tokenizer.texts_to_sequences(x_train)
test_sequences=tokenizer.texts_to_sequences(x_test)

word2idx = tokenizer.word_index
V = len(word2idx)
print(V)

train_data=pad_sequences(train_sequences)

print(train_data.shape)

T=train_data.shape[1]
print(T)

test_data=pad_sequences(test_sequences,maxlen=T)
print(test_data.shape)

D=22 #Embedding dimensionality,Hyperparameter
M=16 #Hidden state dimensionality,hyperparameter
#Note: we actually want to the size of the embedding to (V + 1) x D,
# because the first index starts from 1 and not 0.
# Thus, if the final index of the embedding matrix is V,
# then it actually must have size V + 1.

i=Input(shape=(T,))
x=Embedding(V+1,D)(i)
x=LSTM(M,return_sequences=True)(x)
x=GlobalMaxPooling1D()(x)
x=Dense(1,activation='sigmoid')(x)

model=Model(i,x)

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

r=model.fit(train_data,y_train,validation_data=(test_data,y_test),epochs=10)









